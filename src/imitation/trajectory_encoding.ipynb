{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "First, do basic set up of environment, trajectory generator, expert etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Are we doing real training or not?\n",
    "real_training = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cvtColor'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [6], line 65\u001B[0m\n\u001B[1;32m     47\u001B[0m reward_trainer \u001B[38;5;241m=\u001B[39m preference_comparisons\u001B[38;5;241m.\u001B[39mBasicRewardTrainer(\n\u001B[1;32m     48\u001B[0m     preference_model\u001B[38;5;241m=\u001B[39mpreference_model,\n\u001B[1;32m     49\u001B[0m     loss\u001B[38;5;241m=\u001B[39mpreference_comparisons\u001B[38;5;241m.\u001B[39mCrossEntropyRewardLoss(),\n\u001B[1;32m     50\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m     51\u001B[0m     rng\u001B[38;5;241m=\u001B[39mrng\n\u001B[1;32m     52\u001B[0m )\n\u001B[1;32m     54\u001B[0m agent \u001B[38;5;241m=\u001B[39m PPO(\n\u001B[1;32m     55\u001B[0m     policy\u001B[38;5;241m=\u001B[39mCnnPolicy,\n\u001B[1;32m     56\u001B[0m     env\u001B[38;5;241m=\u001B[39mvenv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     62\u001B[0m     n_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m     63\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m trajectory_generator \u001B[38;5;241m=\u001B[39m \u001B[43mpreference_comparisons\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAgentTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43malgorithm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreward_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreward_net\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexploration_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrng\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# pref_comparisons = preference_comparisons.PreferenceComparisons(\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m#     trajectory_generator,\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m#     reward_net,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;66;03m#     initial_epoch_multiplier=1,\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/PIBBSS/imitation-fork/src/imitation/algorithms/preference_comparisons.py:182\u001B[0m, in \u001B[0;36mAgentTrainer.__init__\u001B[0;34m(self, algorithm, reward_fn, venv, rng, exploration_frac, switch_prob, random_prob, custom_logger)\u001B[0m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# The BufferingWrapper records all trajectories, so we can return\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;66;03m# them after training. This should come first (before the wrapper that\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;66;03m# changes the reward function), so that we return the original environment\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;66;03m# SB3 may move the image-channel dimension in the observation space, making\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;66;03m# `algorithm.get_env()` not match with `reward_fn`.\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffering_wrapper \u001B[38;5;241m=\u001B[39m wrappers\u001B[38;5;241m.\u001B[39mBufferingWrapper(venv)\n\u001B[0;32m--> 182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvenv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward_venv_wrapper \u001B[38;5;241m=\u001B[39m \u001B[43mreward_wrapper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRewardVecEnvWrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffering_wrapper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreward_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_callback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward_venv_wrapper\u001B[38;5;241m.\u001B[39mmake_log_callback()\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malgorithm\u001B[38;5;241m.\u001B[39mset_env(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvenv)\n",
      "File \u001B[0;32m~/Documents/PIBBSS/imitation-fork/src/imitation/rewards/reward_wrapper.py:73\u001B[0m, in \u001B[0;36mRewardVecEnvWrapper.__init__\u001B[0;34m(self, venv, reward_fn, ep_history)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_old_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PIBBSS/imitation-fork/src/imitation/rewards/reward_wrapper.py:84\u001B[0m, in \u001B[0;36mRewardVecEnvWrapper.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 84\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_old_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_old_obs\n",
      "File \u001B[0;32m~/Documents/PIBBSS/imitation-fork/src/imitation/data/wrappers.py:54\u001B[0m, in \u001B[0;36mBufferingWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_transitions \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 54\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_traj_accum \u001B[38;5;241m=\u001B[39m rollout\u001B[38;5;241m.\u001B[39mTrajectoryAccumulator()\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, ob \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(obs):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:58\u001B[0m, in \u001B[0;36mVecFrameStack.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]]:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    Reset all environments\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype:disable=annotation-type-mismatch\u001B[39;00m\n\u001B[1;32m     60\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstackedobs\u001B[38;5;241m.\u001B[39mreset(observation)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:63\u001B[0m, in \u001B[0;36mDummyVecEnv.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvObs:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[0;32m---> 63\u001B[0m         obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_obs(env_idx, obs)\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obs_from_buf()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:83\u001B[0m, in \u001B[0;36mMonitor.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected you to pass keyword argument \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m into reset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_reset_info[key] \u001B[38;5;241m=\u001B[39m value\n\u001B[0;32m---> 83\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/gym/wrappers/time_limit.py:27\u001B[0m, in \u001B[0;36mTimeLimit.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/gym/core.py:292\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/gym/core.py:292\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/gym/core.py:333\u001B[0m, in \u001B[0;36mRewardWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 333\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/gym/core.py:320\u001B[0m, in \u001B[0;36mObservationWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    319\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/imitation/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py:202\u001B[0m, in \u001B[0;36mWarpFrame.observation\u001B[0;34m(self, frame)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobservation\u001B[39m(\u001B[38;5;28mself\u001B[39m, frame: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;124;03m    returns the current observation from a frame\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \n\u001B[1;32m    199\u001B[0m \u001B[38;5;124;03m    :param frame: environment frame\u001B[39;00m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m    :return: the observation\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m     frame \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcvtColor\u001B[49m(frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_RGB2GRAY)\n\u001B[1;32m    203\u001B[0m     frame \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(frame, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheight), interpolation\u001B[38;5;241m=\u001B[39mcv2\u001B[38;5;241m.\u001B[39mINTER_AREA)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m frame[:, :, \u001B[38;5;28;01mNone\u001B[39;00m]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'cvtColor'"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import gym\n",
    "from gym.wrappers import TimeLimit\n",
    "import numpy as np\n",
    "\n",
    "from seals.util import AutoResetWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "from imitation.rewards.reward_nets import CnnRewardNet\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Here we ensure that our environment has constant-length episodes by resetting\n",
    "# it when done, and running until all timesteps have elapsed.\n",
    "# For real training, you will want a much longer time limit than 100\n",
    "def constant_length_asteroids(num_steps):\n",
    "    atari_env = gym.make('AsteroidsNoFrameskip-v4')\n",
    "    preprocessed_env = AtariWrapper(atari_env)\n",
    "    endless_env = AutoResetWrapper(preprocessed_env)\n",
    "    return TimeLimit(endless_env, max_episode_steps=num_steps)\n",
    "\n",
    "if real_training:\n",
    "    venv = make_vec_env(constant_length_asteroids, env_kwargs={\"num_steps\": 100}, n_envs=8)\n",
    "else:\n",
    "    # For real training, you will want a vectorized environment with 8 environments in parallel.\n",
    "    venv = make_vec_env(constant_length_asteroids, env_kwargs={\"num_steps\": 100})\n",
    "venv = VecFrameStack(venv, n_stack=4)\n",
    "\n",
    "reward_net = CnnRewardNet(\n",
    "    venv.observation_space,\n",
    "    venv.action_space,\n",
    ").to(device)\n",
    "\n",
    "# Note that for trajectory encoding we use TotalFragmenter so as to get all possible fragments.\n",
    "fragmenter = preference_comparisons.TotalFragmenter()\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=16,  # To train on atari well, set this to 128\n",
    "    batch_size=16,  # To train on atari well, set this to 256\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=0.00025,\n",
    "    n_epochs=4,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.0,\n",
    "    rng=rng\n",
    ")\n",
    "\n",
    "# pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "#     trajectory_generator,\n",
    "#     reward_net,\n",
    "#     num_iterations=2,\n",
    "#     fragmenter=fragmenter,\n",
    "#     preference_gatherer=gatherer,\n",
    "#     reward_trainer=reward_trainer,\n",
    "#     fragment_length=10,\n",
    "#     transition_oversampling=1,\n",
    "#     initial_comparison_frac=0.1,\n",
    "#     allow_variable_horizon=False,\n",
    "#     initial_epoch_multiplier=1,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a set number of trajectories, then from these we then generate all fragments and use preference_comparison to create a total order."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generation of trajectories and fragmentation done inside the preference comparison function.\n",
    "# and creation of total order\n",
    "num_steps = 100\n",
    "if real_training:\n",
    "    num_steps = 1000\n",
    "trajectories = trajectory_generator.sample(num_steps)\n",
    "fragments = fragmenter(trajectories)\n",
    "\n",
    "# Can't just use 'sorted' since that requires key-generation.\n",
    "# Could easily do bubble or insertion, but O(n^2). Merge better, but requires a little more thought\n",
    "# on how to divide (would require enough comparisons that in worst case it is also O(n^2)).\n",
    "\n",
    "# For the moment will just use bubble since it is easy to implement. Then improve complexity later.\n",
    "def bubble_sorted(array):\n",
    "    for i in range(len(array)):\n",
    "        already_sorted = True\n",
    "        for j in range(len(array)-i-1):\n",
    "            # Feed in the two fragments and compare\n",
    "            if gatherer([array[j], array[j+1]])[0] == 1:\n",
    "                array[j], array[j + 1] = array[j + 1], array[j]\n",
    "                already_sorted = False\n",
    "        if already_sorted:\n",
    "            break\n",
    "    return array\n",
    "\n",
    "fragments_total_order = bubble_sorted(fragments)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
